{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83df00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#Loading the Data\n",
    "training_df = pd.read_csv('data/optdigits.tra', header=None)\n",
    "X_training, y_training = training_df.loc[:, 0:63], training_df.loc[:, 64]\n",
    "X_training.head()\n",
    "\n",
    "testing_df = pd.read_csv('data/optdigits.tes', header=None)\n",
    "X_testing, y_testing = testing_df.loc[:, 0:63], testing_df.loc[:, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b6433e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    ACTIVATION_FUNCS = {\n",
    "        'none': lambda x: x,\n",
    "        'relu': lambda x: max(0, x),\n",
    "        'sigmoid': lambda x: 1 / (1 + np.exp(-x)),\n",
    "        'round': np.around\n",
    "    }\n",
    "    ACTIVATION_FUNCS_DERIVATIVES = {\n",
    "        'none': lambda x: 1,\n",
    "        'relu': lambda x: 0 if x <= 0 else 1,\n",
    "        'sigmoid': lambda x: ACTIVATION_FUNCS['sigmoid'](x) * (1 - ACTIVATION_FUNCS['sigmoid'](x)),\n",
    "        'round': lambda x: 1\n",
    "    }\n",
    "    \n",
    "    def __init__(self, no_of_neurons, activation_function):\n",
    "        self.no_of_neurons = no_of_neurons\n",
    "        self.activation_func = activation_function\n",
    "        self.cached_outputs = np.zeros((self.no_of_neurons,))\n",
    "        \n",
    "    def init_weights_array(self, no_of_previous_layer_neurons):\n",
    "        self.weights = 2 * np.random.rand(self.no_of_neurons, no_of_previous_layer_neurons) - 1\n",
    "        self.biases = 2 * np.random.rand(self.no_of_neurons) - 1\n",
    "        self.clear_errors()\n",
    "        \n",
    "    def clear_errors(self):\n",
    "        self.gradients = np.zeros((self.no_of_neurons, self.weights.shape[1]))\n",
    "        self.derivatives = np.zeros((self.no_of_neurons,))\n",
    "        \n",
    "    def get_outputs(self, previous_layer_outputs):\n",
    "        outputs = np.zeros((self.no_of_neurons,))\n",
    "        for neuron_idx in range(self.no_of_neurons):\n",
    "            #print(\"W: \",self.weights[neuron_idx])\n",
    "            #print(\"B: \",self.biases[neuron_idx])\n",
    "            #print(\"Prevlayeroutputs: \", previous_layer_outputs)\n",
    "            activation_input = np.dot(self.weights[neuron_idx], previous_layer_outputs) + self.biases[neuron_idx]\n",
    "            neuron_output = Layer.ACTIVATION_FUNCS[self.activation_func](activation_input)\n",
    "            #print(\"Output: \", neuron_output)\n",
    "            outputs[neuron_idx] = neuron_output\n",
    "        self.cached_outputs = outputs\n",
    "        return outputs\n",
    "        \n",
    "    def accumulate_error(self, neuron_derivatives, previous_layer_outputs):\n",
    "        prev_layer_derivatives = np.zeros((previous_layer_outputs.shape[0],))\n",
    "        activation_func_derivatives = np.vectorize(Layer.ACTIVATION_FUNCS_DERIVATIVES[self.activation_func])(self.cached_outputs)\n",
    "        for neuron_idx in range(self.no_of_neurons):\n",
    "            activation_func_derivative = activation_func_derivatives[neuron_idx]\n",
    "            gradient = activation_func_derivative * neuron_derivatives[neuron_idx] * previous_layer_outputs\n",
    "            self.gradients[neuron_idx] += gradient\n",
    "            \n",
    "            for prev_layer_neuron_idx in range(previous_layer_outputs.shape[0]):\n",
    "                self.derivatives[neuron_idx] += activation_func_derivative * neuron_derivatives[neuron_idx]\n",
    "                prev_layer_derivatives[prev_layer_neuron_idx] += activation_func_derivative * self.weights[neuron_idx][prev_layer_neuron_idx] * neuron_derivatives[neuron_idx]\n",
    "        \n",
    "        return prev_layer_derivatives\n",
    "            \n",
    "class InputLayer(Layer):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__(input_size, \"none\")\n",
    "        \n",
    "    def init_weights_array(self):\n",
    "        self.weights = np.ones((self.no_of_neurons,))\n",
    "        self.biases = np.zeros((self.no_of_neurons,))\n",
    "            \n",
    "class NeuralNetwork:\n",
    "    \n",
    "    ERROR_FUNCTIONS = {\n",
    "        \"mse\": lambda y, yhat: 0.5 * (y - yhat) ** 2\n",
    "    }\n",
    "    ERROR_FUNCTIONS_DERIVATIVES = {\n",
    "        \"mse\": lambda y, yhat: yhat - y\n",
    "    }\n",
    "    \n",
    "    def __init__(self, layers, error_function=\"mse\"):\n",
    "        self.layers = layers\n",
    "        self.error_function = error_function\n",
    "        self.layers[0].init_weights_array()\n",
    "        for i in range(1, len(layers)):\n",
    "            self.layers[i].init_weights_array(self.layers[i - 1].no_of_neurons)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        last_outputs = X\n",
    "        self.layers[0].cached_outputs = X\n",
    "        for i in range(1, len(self.layers)):\n",
    "            last_outputs = self.layers[i].get_outputs(last_outputs)\n",
    "        return last_outputs\n",
    "    \n",
    "    def train(self, X, y, learning_rate = 1e-06, batch_size=1):\n",
    "        iter_no = 0\n",
    "        for idx in range(len(X)):\n",
    "            iter_no += 1\n",
    "            \n",
    "            x_sample = X[idx]\n",
    "            y_sample = y[idx]\n",
    "            yhat = self.predict(x_sample)\n",
    "            derivatives = Network.ERROR_FUNCTIONS_DERIVATIVES[self.error_function](y_sample, yhat)\n",
    "            for i in range(len(self.layers) - 1, 0, -1):\n",
    "                derivatives = self.layers[i].accumulate_error(derivatives, self.layers[i - 1].cached_outputs)\n",
    "                \n",
    "            if iter_no == batch_size:\n",
    "                for i in range(len(self.layers) - 1, 0, -1):\n",
    "                    self.layers[i].weights -= learning_rate * self.layers[i].gradients\n",
    "                    self.layers[i].biases -= learning_rate * self.layers[i].derivatives\n",
    "                    self.layers[i].clear_errors()\n",
    "                iter_no = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "162426ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_COUNT = 1\n",
    "\n",
    "autoencoder = NeuralNetwork([InputLayer(64), Layer(16, \"none\"), Layer(16, \"none\"), Layer(64, \"round\")])\n",
    "for i in range(EPOCH_COUNT):\n",
    "    autoencoder.train(X_training.values, X_training.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "024052b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.  0.  3.  4. -4.  5.  5. -1. -5.  4. 13. -7.  8. -2.  1. -3.  0.  7.\n",
      "  7. -5.  5.  7.  4.  6.  3. 10.  1.  3. -1.  8.  9.  6. -1.  9. 10.  3.\n",
      "  2.  7. 13. -2. -1.  6.  8. -4. -5. 12.  6.  0.  1.  3. 10.  0.  2.  6.\n",
      "  4. -4. -4.  4.  7.  3.  7. -5. -2. -2.]\n"
     ]
    }
   ],
   "source": [
    "print(X_training.values[1] - autoencoder.predict(X_training.values[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
